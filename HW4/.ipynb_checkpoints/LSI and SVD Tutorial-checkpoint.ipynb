{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "In natural language understanding (NLU) tasks, there is a hierarchy of lenses through which we can extract meaning — from words to sentences to paragraphs to documents. At the document level, one of the most useful ways to understand text is by analyzing its topics. The process of learning, recognizing, and extracting these topics across a collection of documents is called topic modeling.\n",
    "\n",
    "Topic modeling is a text mining technique which provides methods for identifying co-occurring keywords to summarize large collections of textual information. It helps in discovering hidden topics in the document, annotate the documents with these topics, and organize a large amount of unstructured data. Topic Modeling automatically discover the hidden themes from given documents. It is an unsupervised text analytics algorithm that is used for finding the group of words from the given document. These group of words represents a topic. There is a possibility that, a single document can associate with multiple themes. For example, a group words such as 'patient', 'doctor', 'disease', 'cancer', ad 'health' will represents topic 'healthcare'. Topic Modeling is a different game compared to rule-based text searching that uses regular expressions.\n",
    "\n",
    "<img src=\"images/image_1.png\" alt=\"Topic Modeling\" style=\"width: 80%\"/>\n",
    "\n",
    "\n",
    "#### Comparison Between Text Classification and topic modeling\n",
    "\n",
    "Text classification is a supervised machine learning problem, where a text document or article classified into a pre-defined set of classes. Topic modeling is the process of discovering groups of co-occurring words in text documents. These group co-occurring related words makes \"topics\". It is a form of unsupervised learning, so the set of possible topics are unknown. Topic modeling can be used to solve the text classification problem. Topic modeling will identify the topics presents in a document\" while text classification classifies the text into a single class.\n",
    "\n",
    "<img src=\"images/image_2.png\" alt=\"Topic Modeling vs Image Classification\" style=\"width: 80%\"/>\n",
    "\n",
    "\n",
    "# Latent Semantic Index (LSI)\n",
    "\n",
    "LSI (Latent Semantic Index) also known as LSA (Latent Semantic Analysis). LSI uses bag of word (BoW) model, which results in a term-document matrix (occurrence of terms in a document). Rows represent terms and columns represent documents. LSA learns latent topics by performing a matrix decomposition on the document-term matrix using Singular value decomposition (SVD). LSI is typically used as a dimension reduction or noise reducing technique.\n",
    "\n",
    "<img src=\"images/image_3.png\" alt=\"Latent Semantic Index\" style=\"width: 80%\"/>\n",
    "\n",
    "\n",
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "SVD is a matrix factorization method that represents a matrix in the product of two matrices. More concretely, the SVD algorithm factorizes a matrix into one matrix with orthogonal columns and one with orthogonal rows (along with a diagonal matrix, which contains the relative importance of each factor). SVD is an exact decomposition, since the matrices it creates are big enough to fully cover the original matrix. SVD is extremely widely used in linear algebra, and specifically in data science.\n",
    "\n",
    "$$\n",
    "M = U \\Sigma V^\\ast\n",
    "$$\n",
    "\n",
    "- $M$ is an m×m matrix\n",
    "- $U$ is a m×n left singular matrix\n",
    "- $\\Sigma$ is a n×n diagonal matrix with non-negative real numbers.\n",
    "- $V$ is a m×n right singular matrix\n",
    "- $V^\\ast$ is n×m matrix, which is the transpose of the V.\n",
    "\n",
    "\n",
    "# Implementation of LSI using SVD\n",
    "\n",
    "We will now take a dataset of documents in several different categories, and find topics (consisting of groups of words) for them. Knowing the actual categories helps us evaluate if the topics we find make sense. We will try to achieve this by the SVD matrix factorization method.\n",
    "\n",
    "For this task, we will be using the `20 Newsgroups` dataset. Newsgroups are discussion groups on Usenet, which was popular in the 80s and 90s before the web really took off. This dataset includes 18,000 newsgroups posts with 20 topics. [(Dataset Usage details - Scikit Learn)](https://scikit-learn.org/stable/datasets/#the-20-newsgroups-text-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import decomposition\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034,) (2034,)\n"
     ]
    }
   ],
   "source": [
    "# Setup Data\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)\n",
    "\n",
    "print(newsgroups_train.filenames.shape, newsgroups_train.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n",
      "\n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n",
      "['comp.graphics' 'talk.religion.misc' 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "# Explore Data\n",
    "print(\"\\n\".join(newsgroups_train.data[:3]))\n",
    "\n",
    "print(np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 2 0 2 0 2 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.target[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The target attribute is the integer index of the category.\n",
    "\n",
    "#### Feature extraction\n",
    "We will now extract features like word count and the term frequency-inverse document frequency (tf-idf) from the dataset. For this we will use Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 (2034, 26576)\n",
      "(26576,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['brow', 'brown', 'browning', 'browns', 'browse', 'browsing',\n",
       "       'bruce', 'bruces', 'bruise', 'bruised', 'bruises', 'brunner',\n",
       "       'brush', 'brushmapping', 'brushmaps', 'brussel', 'brutal',\n",
       "       'brutally', 'brute', 'bryan'], dtype='<U80')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import relevant helper functions\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Extract features\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data).todense() # (documents, vocab)\n",
    "\n",
    "print(len(newsgroups_train.data), vectors.shape)\n",
    "\n",
    "# Generate vocabulary\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "# Visualize vocabulary\n",
    "print(vocab.shape)\n",
    "vocab[5000:5020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Matrix Factorization\n",
    "\n",
    "We will now apply SVD to our feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD\n",
    "U, s, Vh = linalg.svd(vectors, full_matrices=False)\n",
    "\n",
    "# Visualize the shape of each of the output variable\n",
    "print(U.shape, s.shape, Vh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the top 8 words for each topics\n",
    "num_top_words = 8\n",
    "\n",
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = [top_words(t) for t in a]\n",
    "    return [' '.join(t) for t in topic_words]\n",
    "\n",
    "# Function call\n",
    "show_topics(Vh[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see that we get topics that match the kinds of clusters we would expect! This is despite the fact that this is an unsupervised algorithm - which is to say, we never actually told the algorithm how our documents are grouped."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
