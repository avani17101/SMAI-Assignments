{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization using Gradient Descent\n",
    "\n",
    "In this excercise, you are required to implement matrix factorization method, specifically [Non-Negative Matrix Factorization (NMF)](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization), using gradient descent. You have to apply the matrix factorization to solve topic modeling. \n",
    "\n",
    "(Please refer to the tutorial on basics of topic modeling, LSI with SVD (Tutorial Set 4), for details on LSI)\n",
    "\n",
    "## Applying NMF to solve Topic Modeling\n",
    "Given a term document matrix $V$, NMF factorizes it into two matrix $W$ and $H$ with the property that all three documents have no negative elements.\n",
    "<img src=\"content/NMF.png\" alt=\"Non-negative matrix factorization\" style=\"width: 80%\">\n",
    "\n",
    "In Non-negative Matrix Factorization, a document-term matrix is approximately factorized into term-feature and feature-document matrices.\n",
    "\n",
    "$V = WH$ Matrix multiplication can be implemented as computing the column vectors of $V$ as linear combinations of the basis vectors (column vectors) in $W$ (or the topics discovered from the documents) using coefficients supplied by columns of $H$ (or the membership weights for the topics in each document). That is, each column of V can be computed as follows:\n",
    "$$ v_i = W h_i$$\n",
    "\n",
    "In what follows, we will first see an example of applying NMF by using [SKLearn NMF API](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) for the task of topic modeling. Later you will be required to implement NMF using gradient descent.\n",
    "\n",
    "### Scikit-Learn implementation of NMF for topic modeling\n",
    "Given a set of multivariate  $n$-dimensional data vectors, they are put into an  $n\\times m$  matrix  $V$  as its columns, where  $m$  is the number of examples in the data set. This matrix  $V$  is approximately factorized into an  $n \\times t$  matrix  $W$  and an  $t \\times m$  matrix  $H$ , where  $t$  is generally less than  $n$  or  $m$ . Hence, this results in a compression of the original data matrix.\n",
    "\n",
    "In terms of topic modeling, the input document-term matrix  $V$  is factorized into a  $n \\times t$  document-topic matrix and a  $t \\times m$  topic-term matrix, where  $t$  is the number of topics produced. Similar to tutorial 4, we will be using 20 NewsFetch dataset for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Avani Gupta <br>\n",
    "Roll: 2019121004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute document features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english')\n",
    "vectors_tfidf = vectorizer_tfidf.fit_transform(newsgroups_train.data).todense() # (documents, vocab)\n",
    "vectors_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute NMF using Scikit Learn library\n",
    "\n",
    "We will also write a function to display top 8 words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words=8\n",
    "vocab = np.array(vectorizer_tfidf.get_feature_names())\n",
    "\n",
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "d = 5 # num topics\n",
    "clf = decomposition.NMF(n_components=d, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = clf.fit_transform(vectors_tfidf)\n",
    "H1 = clf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people don think just like objective say morality',\n",
       " 'graphics thanks files image file program windows know',\n",
       " 'space nasa launch shuttle orbit moon lunar earth',\n",
       " 'ico bobbe tek beauchaine bronx manhattan sank queens',\n",
       " 'god jesus bible believe christian atheism does belief']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF using SGD\n",
    "\n",
    "In stochastic gradient descent (SGD), we evaluate our loss function on just a sample of our data (sometimes called a mini-batch). We would get different loss values on different samples of the data, so this is why it is stochastic. It turns out that this is still an effective way to optimize, and it's much more efficient!\n",
    "\n",
    "### Applying SGD to NMF\n",
    "\n",
    "Goal: Decompose $V\\;(m \\times n)$ into\n",
    "$$ V \\approx HW$$\n",
    "where $W\\;(m \\times d)$ and $H\\;(d \\times n)$, $W,\\;H\\; \\geq \\;0$, and we've minimized the Frobenius norm of $V-WH$. The objective function can therefore be written as the following:\n",
    "$$\n",
    "\\min_{H \\geq 0, W \\geq 0} F(H,W) = \\frac{1}{2} ||V - HW||^{2} + \\frac{\\lambda}{2} \\left( ||H||^2 + ||W||^2 \\right)\n",
    "$$\n",
    "\n",
    "### Implementation of NMF using SGD (Excercise)\n",
    "__Approach:__ Given the objective function above, pick random positive $W$ & $H$, and then use SGD to optimize. \n",
    "\n",
    "(Note that the objective function is non-convex in nature, and is convex only if we consider $H$ and $W$ separately. You can directly write the gradient descent rule for the objective function presented above)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial\n",
      "norm: 44.4264794074542 min W: 6.240378201888802e-07 min H: 5.099848891127229e-08 num of negative W: 0 num of negative H: 0\n",
      "after first update\n",
      "norm: 44.41859490841542 min W: -0.000880985585715645 min H: -8.833662674790848e-05 num of negative W: 139 num of negative H: 290\n",
      "iter  0\n",
      "norm: 44.412916654420606 min W: -0.0007093268082925707 min H: -7.769420423041111e-05 num of negative W: 111 num of negative H: 262\n",
      "iter  50\n",
      "norm: 44.24419771663191 min W: -0.00013440935286741776 min H: -0.00013246610552814802 num of negative W: 36 num of negative H: 2966\n",
      "iter  100\n",
      "norm: 44.14123536919818 min W: -0.00018773958445390292 min H: -0.0001782590480765164 num of negative W: 58 num of negative H: 6898\n",
      "iter  150\n",
      "norm: 44.08253742014953 min W: -0.0015134271088304734 min H: -0.00040749337037090873 num of negative W: 123 num of negative H: 8738\n",
      "iter  200\n",
      "norm: 43.988354915062494 min W: -0.0034952934096576763 min H: -0.0008306918047459974 num of negative W: 133 num of negative H: 9339\n",
      "iter  250\n",
      "norm: 43.888554331595216 min W: -0.002393720296999601 min H: -0.0019987564654250453 num of negative W: 153 num of negative H: 9165\n",
      "iter  300\n",
      "norm: 43.82796912525586 min W: -0.0023713140981512044 min H: -0.0020632679770173972 num of negative W: 132 num of negative H: 9231\n",
      "iter  350\n",
      "norm: 43.79260297968698 min W: -0.00114525784300281 min H: -0.0018851871749999607 num of negative W: 145 num of negative H: 9279\n",
      "iter  400\n",
      "norm: 43.76407160999864 min W: -0.0019843929489859005 min H: -0.0025475254802514295 num of negative W: 140 num of negative H: 8667\n",
      "iter  450\n",
      "norm: 43.74979086890655 min W: -0.00203174019902906 min H: -0.0037246035182069634 num of negative W: 91 num of negative H: 8449\n",
      "iter  500\n",
      "norm: 43.737468001718135 min W: -0.0012615573675764517 min H: -0.004525121990783562 num of negative W: 145 num of negative H: 8686\n",
      "iter  550\n",
      "norm: 43.731658783556924 min W: -0.0015411884453720517 min H: -0.0023718608986766088 num of negative W: 151 num of negative H: 8773\n",
      "iter  600\n",
      "norm: 43.72895315803146 min W: -0.0015209939759975796 min H: -0.0009155672178747789 num of negative W: 158 num of negative H: 8962\n",
      "iter  650\n",
      "norm: 43.726166157394694 min W: -0.0020184933822347145 min H: -0.0034101032773781927 num of negative W: 147 num of negative H: 8862\n",
      "iter  700\n",
      "norm: 43.72432597594044 min W: -0.0012639009978697442 min H: -0.003526005434519244 num of negative W: 195 num of negative H: 8625\n",
      "iter  750\n",
      "norm: 43.72237862901216 min W: -0.0008508787349436866 min H: -0.005070726452501557 num of negative W: 142 num of negative H: 8565\n",
      "iter  800\n",
      "norm: 43.722126079506495 min W: -0.0008366421333868061 min H: -0.0025171257890431037 num of negative W: 147 num of negative H: 8621\n",
      "iter  850\n",
      "norm: 43.72066077104355 min W: -0.001290543542200207 min H: -0.0018595787302458293 num of negative W: 153 num of negative H: 8679\n",
      "iter  900\n",
      "norm: 43.72137437260892 min W: -0.0009188729763075796 min H: -0.002061065083624154 num of negative W: 182 num of negative H: 8884\n",
      "iter  950\n",
      "norm: 43.72052239569433 min W: -0.0010771935787239786 min H: -0.0014696187688075313 num of negative W: 183 num of negative H: 9148\n",
      "topics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['think don people just objective like morality moral',\n",
       " 'ico bobbe tek beauchaine bronx manhattan sank queens',\n",
       " 'god jesus bible believe christian atheism does belief',\n",
       " 'thanks graphics files image file program windows know',\n",
       " 'space nasa launch shuttle orbit moon lunar earth']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' WRITE YOUR CODE BELOW '''\n",
    "mu = 1e-6\n",
    "lambdaa=1e3\n",
    "lr=1e-2\n",
    "rows, cols = vectors_tfidf.shape\n",
    "def updateGrads(M, W, H,lr):\n",
    "    R = W@H-M\n",
    "    dW = R@H.T\n",
    "    dW += np.where(W>=mu,0, np.min(W - mu, 0))*lambdaa\n",
    "    dH = W.T@R\n",
    "    dH += np.where(H>=mu,0, np.min(H - mu, 0))*lambdaa \n",
    "    W -= lr*dW\n",
    "    H -= lr*dH\n",
    "    \n",
    "\n",
    "def printvals(M,W,H):\n",
    "    negW = W<0\n",
    "    negH = H<0\n",
    "    negWnum = negW.sum()\n",
    "    negHnum = negH.sum()\n",
    "    norm = np.linalg.norm(M-W@H)\n",
    "    print(\"norm:\",norm, \"min W:\",W.min(), \"min H:\", H.min(), \"num of negative W:\",negWnum ,\"num of negative H:\",negHnum)\n",
    "    \n",
    "W = np.random.normal(scale=0.01, size=(rows,d))\n",
    "W = np.abs(W)\n",
    "H = np.random.normal(scale=0.01, size=(d,cols))\n",
    "H = np.abs(H) \n",
    "print(\"initial\")\n",
    "printvals(vectors_tfidf, W, H)\n",
    "updateGrads(vectors_tfidf,W,H,lr)\n",
    "print(\"after first update\")\n",
    "printvals(vectors_tfidf, W, H)\n",
    "\n",
    "\n",
    "for i in range(1000): \n",
    "    updateGrads(vectors_tfidf,W,H,lr)\n",
    "    if i % 50 == 0:\n",
    "        print(\"iter \",i)\n",
    "        printvals(vectors_tfidf, W, H)\n",
    "\n",
    "print(\"topics\")        \n",
    "show_topics(H)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
